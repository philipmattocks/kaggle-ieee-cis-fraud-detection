{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT_ID=frauddetectionkaggle\n",
      "env: BUCKET_ID=frauddetectionkagglepkmatt\n",
      "env: JOB_DIR=gs://frauddetectionkagglepkm/xgb_job_dir\n",
      "env: REGION=europe-west1\n",
      "env: TRAINER_PACKAGE_PATH=./fraud_detection_hp_tuning\n",
      "env: MAIN_TRAINER_MODULE=fraud_detection_hp_tuning.train\n",
      "env: RUNTIME_VERSION=1.14\n",
      "env: PYTHON_VERSION=3.5\n",
      "env: HPTUNING_CONFIG=hptuning_config.yaml\n",
      "env: MODEL_NAME=fraud_detection_hp_tuning\n",
      "mkdir: cannot create directory ‘fraud_detection_hp_tuning’: File exists\n"
     ]
    }
   ],
   "source": [
    "%env PROJECT_ID frauddetectionkaggle\n",
    "%env BUCKET_ID frauddetectionkagglepkmatt\n",
    "%env JOB_DIR gs://frauddetectionkagglepkmatt/xgb_job_dir\n",
    "%env REGION europe-west1\n",
    "%env TRAINER_PACKAGE_PATH ./fraud_detection_hp_tuning\n",
    "%env MAIN_TRAINER_MODULE fraud_detection_hp_tuning.train\n",
    "%env RUNTIME_VERSION 1.14\n",
    "%env PYTHON_VERSION 3.5\n",
    "%env HPTUNING_CONFIG hptuning_config.yaml\n",
    "%env MODEL_NAME fraud_detection_hp_tuning\n",
    "! mkdir fraud_detection_hp_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./fraud_detection_hp_tuning/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./fraud_detection_hp_tuning/train.py\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "import hypertune\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.externals import joblib\n",
    "from random import shuffle\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "identity = 'train_identity_small.csv'\n",
    "transaction = 'train_transaction_small.csv'\n",
    "\n",
    "BUCKET_ID = 'frauddetectionkagglepkmatt'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--job-dir',  # handled automatically by AI Platform\n",
    "    help='GCS location to write checkpoints and export models',\n",
    "    required=True\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--max_depth',  # Specified in the config file\n",
    "    help='Maximum depth of the XGBoost tree. default: 3',\n",
    "    default=3,\n",
    "    type=int\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--n_estimators',  # Specified in the config file\n",
    "    help='Number of estimators to be created. default: 100',\n",
    "    default=100,\n",
    "    type=int\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--booster',  # Specified in the config file\n",
    "    help='which booster to use: gbtree, gblinear or dart. default: gbtree',\n",
    "    default='gbtree',\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--learning_rate',  # Specified in the config file\n",
    "    help='what learning_rate to use: 0.05 typical',\n",
    "    default=0.05,\n",
    "    type=float\n",
    ")\n",
    "\n",
    "args = parser.parse_args()\n",
    "#  bucket holding the data\n",
    "bucket = storage.Client().bucket(BUCKET_ID)\n",
    "\n",
    "# Path to the data inside the public bucket\n",
    "data_dir = 'data/raw/'\n",
    "\n",
    "if not os.path.exists(identity):\n",
    "    # Download the data\n",
    "    blob = bucket.blob(''.join([data_dir, identity]))\n",
    "    blob.download_to_filename(identity)\n",
    "    \n",
    "if not os.path.exists(transaction):    \n",
    "    blob = bucket.blob(''.join([data_dir, transaction]))\n",
    "    blob.download_to_filename(transaction)\n",
    "\n",
    "\n",
    "def load_and_merge_data(transaction_csv,identity_csv,isTrain,nrows=1000000):\n",
    "    df_transaction = pd.read_csv(transaction_csv, index_col='TransactionID',nrows=nrows)\n",
    "    df_identity = pd.read_csv(identity_csv, index_col='TransactionID',nrows=nrows)\n",
    "    df = pd.merge(df_transaction, df_identity, on='TransactionID', how='left')\n",
    "    del df_transaction\n",
    "    del df_identity\n",
    "    if isTrain:\n",
    "        labels = df[['isFraud']]\n",
    "        df.pop('isFraud')\n",
    "    else:\n",
    "        labels = []\n",
    "    return df, labels\n",
    "\n",
    "train,labels  = load_and_merge_data(transaction,identity,isTrain=True)\n",
    "#train,labels  = load_and_merge_data('gs://frauddetectionkagglepkmatt/data/raw/train_transaction.csv','gs://frauddetectionkagglepkmatt/data/raw/train_identity.csv',isTrain=True,nrows=5000)\n",
    "# #validate,vallabels  = load_and_merge_data('./data/raw/test_transaction.csv','./data/raw/test_identity.csv',isTrain=False,nrows=5000)\n",
    "\n",
    "#print(train.shape)\n",
    "\n",
    "def get_lists_of_numerical_categorical(df,regex):\n",
    "    #Regex for categorical fields:\n",
    "    categorical = []\n",
    "    numerical = []\n",
    "\n",
    "    #Create lists of categorical and numeircal fields:\n",
    "    for i in df:\n",
    "        if re.match(regex, i):\n",
    "            categorical.append(i)\n",
    "        else:\n",
    "            numerical.append(i)\n",
    "    return numerical,categorical\n",
    "\n",
    "cat_columns_regex='ProductCD|card[1-6]|addr\\d|\\w_emaildomain|M[1-9]|time_|Device\\w+|id_12|id_13|id_14|id_15|id_16|id_17|id_18|id_19|id_20|id_21|id_22|id_23|id_24|id_25|id_26|id_27|id_28|id_29|id_30|id_31|id_32|id_33|id_34|id_35|id_36|id_37|id_38'\n",
    "numerical,categorical = get_lists_of_numerical_categorical(train,cat_columns_regex)\n",
    "\n",
    "def numerically_encode_string_categoricals(df):\n",
    "    for i in df.columns:\n",
    "        if df[i].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(df[i].values) + list(df[i].values))\n",
    "            df[i] = lbl.transform(list(df[i].values))\n",
    "    return df\n",
    "train = numerically_encode_string_categoricals(train)\n",
    "#validate = numerically_encode_string_categoricals(validate)\n",
    "\n",
    "# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "# WARNING! THIS CAN DAMAGE THE DATA \n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "\n",
    "#Impute median for numerical and mode for categorical\n",
    "def impute_cat_and_num(df,numerical,categorical):\n",
    "    fill_NaN_numerical = Imputer(missing_values=np.nan, strategy='median',axis=1)\n",
    "    fill_NaN_categorical = Imputer(missing_values=np.nan, strategy='most_frequent',axis=1)\n",
    "    df[numerical] = fill_NaN_numerical.fit_transform(df[numerical])\n",
    "    df[categorical] = fill_NaN_categorical.fit_transform(df[categorical])\n",
    "    return df\n",
    "train = impute_cat_and_num(train,numerical,categorical)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.2,random_state=42)\n",
    "\n",
    "# Create the regressor, here we will use a Lasso Regressor to demonstrate the use of HP Tuning.\n",
    "# Here is where we set the variables used during HP Tuning from\n",
    "# the parameters passed into the python script\n",
    "\n",
    "#here are the tricks\n",
    "#usually max_depth is 6,7,8\n",
    "#learning rate is around 0.05, but small changes may make big diff\n",
    "#tuning min_child_weight subsample colsample_bytree can have \n",
    "#much fun of fighting against overfit \n",
    "#n_estimators is how many round of boosting\n",
    "#finally, ensemble xgboost with multiple seeds may reduce variance\n",
    "\n",
    "\n",
    "clf = xgb.XGBClassifier(max_depth=args.max_depth,\n",
    "                             n_estimators=args.n_estimators,\n",
    "                             booster=args.booster,\n",
    "                             nthread=7,\n",
    "                             learning_rate=args.learning_rate\n",
    "                            )\n",
    "\n",
    "#clf = xgb.XGBClassifier()\n",
    "\n",
    "# Transform the features and fit them to the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Calculate the mean accuracy on the given test data and labels.\n",
    "score = clf.score(X_train, y_train)\n",
    "\n",
    "# The default name of the metric is training/hptuning/metric. \n",
    "# We recommend that you assign a custom name. The only functional difference is that \n",
    "# if you use a custom name, you must set the hyperparameterMetricTag value in the \n",
    "# HyperparameterSpec object in your job request to match your chosen name.\n",
    "# https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#HyperparameterSpec\n",
    "hpt = hypertune.HyperTune()\n",
    "hpt.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag='my_metric_tag',\n",
    "    metric_value=score,\n",
    "    global_step=1000)\n",
    "\n",
    "# Export the model to a file\n",
    "model_filename = 'model.pkl'\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "# Example: job_dir = 'gs://BUCKET_ID/xgboost_job_dir/1'\n",
    "job_dir =  args.job_dir.replace('gs://', '')  # Remove the 'gs://'\n",
    "# Get the Bucket Id\n",
    "bucket_id = job_dir.split('/')[0]\n",
    "# Get the path\n",
    "bucket_path = job_dir[len('{}/'.format(bucket_id)):]  # Example: 'xgboost_job_dir/1'\n",
    "# Upload the model to GCS\n",
    "bucket = storage.Client().bucket(bucket_id)\n",
    "blob = bucket.blob('{}/{}'.format(\n",
    "    bucket_path,\n",
    "    model_filename))\n",
    "\n",
    "blob.upload_from_filename(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./fraud_detection_hp_tuning/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./fraud_detection_hp_tuning/__init__.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2018 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Note that __init__.py can be an empty file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./hptuning_config.yaml\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2018 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# hyperparam.yaml\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 1\n",
    "    maxParallelTrials: 5\n",
    "    hyperparameterMetricTag: my_metric_tag\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: max_depth\n",
    "      type: INTEGER\n",
    "      minValue: 3\n",
    "      maxValue: 8\n",
    "    - parameterName: n_estimators\n",
    "      type: INTEGER\n",
    "      minValue: 50\n",
    "      maxValue: 200\n",
    "    - parameterName: booster\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"gbtree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./setup.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2018 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['cloudml-hypertune']\n",
    "\n",
    "setup(\n",
    "    name='fraud_detecion',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Auto MPG XGBoost HP tuning training application'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 100.0%\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "xgb_job_dir/model.pkl\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform local train \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --package-path $TRAINER_PACKAGE_PATH \\\n",
    "  --module-name $MAIN_TRAINER_MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_filename = 'model.pkl'\n",
    "clf = pickle.load( open( model_filename, \"rb\" ) )\n",
    "# Example: job_dir = 'gs://BUCKET_ID/xgboost_job_dir/1'\n",
    "#job_dir =  args.job_dir.replace('gs://', '')  # Remove the 'gs://'\n",
    "job_dir =  'gs://frauddetectionkagglepkm/xgb_job_dir'.replace('gs://', '')  # Remove the 'gs://'\n",
    "# Get the Bucket Id\n",
    "bucket_id = job_dir.split('/')[0]\n",
    "# Get the path\n",
    "bucket_path = job_dir[len('{}/'.format(bucket_id)):]  # Example: 'xgboost_job_dir/1'\n",
    "# Upload the model to GCS\n",
    "bucket = storage.Client().bucket(bucket_id)\n",
    "blob = bucket.blob('{}/{}'.format(\n",
    "    bucket_path,\n",
    "    model_filename))\n",
    "\n",
    "blob.upload_from_filename(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=None, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.05, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=nan, n_estimators=100, n_jobs=1,\n",
       "       nthread=7, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1, verbosity=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":/home/jupyter/kaggle-ieee-cis-fraud-detection/GCP work/Hyperparameter_Tuning/fraud_detection_hp_tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] --job-dir JOB_DIR [--max_depth MAX_DEPTH]\n",
      "                [--n_estimators N_ESTIMATORS] [--booster BOOSTER]\n",
      "train.py: error: unrecognized arguments: --config hptuning_config.yaml\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b\"# Setup python so it sees the task module which controls the model.py\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/${MODEL_NAME}\\necho ${PYTHONPATH}\\n# Currently set for python 3.  To run with python 2\\n#    1.  Replace 'python3' with 'python' in the following command\\n#    2.  Edit trainer/task.py to reflect proper module import method \\npython3 -m $MAIN_TRAINER_MODULE \\\\\\n  --job-dir $JOB_DIR  \\\\\\n  --config $HPTUNING_CONFIG\\n\"' returned non-zero exit status 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-aeb8723b58a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"# Setup python so it sees the task module which controls the model.py\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/${MODEL_NAME}\\necho ${PYTHONPATH}\\n# Currently set for python 3.  To run with python 2\\n#    1.  Replace 'python3' with 'python' in the following command\\n#    2.  Edit trainer/task.py to reflect proper module import method \\npython3 -m $MAIN_TRAINER_MODULE \\\\\\n  --job-dir $JOB_DIR  \\\\\\n  --config $HPTUNING_CONFIG\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</usr/local/lib/python3.5/dist-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b\"# Setup python so it sees the task module which controls the model.py\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/${MODEL_NAME}\\necho ${PYTHONPATH}\\n# Currently set for python 3.  To run with python 2\\n#    1.  Replace 'python3' with 'python' in the following command\\n#    2.  Edit trainer/task.py to reflect proper module import method \\npython3 -m $MAIN_TRAINER_MODULE \\\\\\n  --job-dir $JOB_DIR  \\\\\\n  --config $HPTUNING_CONFIG\\n\"' returned non-zero exit status 2"
     ]
    }
   ],
   "source": [
    " %%bash\n",
    " # Setup python so it sees the task module which controls the model.py\n",
    " export PYTHONPATH=${PYTHONPATH}:${PWD}/${MODEL_NAME}\n",
    " echo ${PYTHONPATH}\n",
    " # Currently set for python 3.  To run with python 2\n",
    " #    1.  Replace 'python3' with 'python' in the following command\n",
    " #    2.  Edit trainer/task.py to reflect proper module import method \n",
    " python3 -m $MAIN_TRAINER_MODULE \\\n",
    "   --job-dir $JOB_DIR  \\\n",
    "   --config $HPTUNING_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [fraud_detection_hp_tuning_20190917_153340] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe fraud_detection_hp_tuning_20190917_153340\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs fraud_detection_hp_tuning_20190917_153340\n",
      "jobId: fraud_detection_hp_tuning_20190917_153340\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform jobs submit training fraud_detection_hp_tuning_$(date +\"%Y%m%d_%H%M%S\") \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --package-path $TRAINER_PACKAGE_PATH \\\n",
    "  --module-name $MAIN_TRAINER_MODULE \\\n",
    "  --region $REGION \\\n",
    "  --runtime-version=$RUNTIME_VERSION \\\n",
    "  --python-version=$PYTHON_VERSION \\\n",
    "  --scale-tier basic \\\n",
    "  --config $HPTUNING_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.local.train) unrecognized arguments:\n",
      "  --config (did you mean '--configuration'?)\n",
      "  hptuning_config.yaml\n",
      "  To search the help text of gcloud commands, run:\n",
      "  gcloud help -- SEARCH_TERMS\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform local train \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --package-path $TRAINER_PACKAGE_PATH \\\n",
    "  --module-name $MAIN_TRAINER_MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":/home/jupyter/kaggle-ieee-cis-fraud-detection/fraud_detection_hp_tuning\n",
      "Memory usage of dataframe is 1.65 MB\n",
      "Memory usage after optimization is: 0.40 MB\n",
      "Decreased by 75.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Setup python so it sees the task module which controls the model.py\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/${MODEL_NAME}\n",
    "echo ${PYTHONPATH}\n",
    "# Currently set for python 3.  To run with python 2\n",
    "#    1.  Replace 'python3' with 'python' in the following command\n",
    "#    2.  Edit trainer/task.py to reflect proper module import method \n",
    "python3 -m $MAIN_TRAINER_MODULE \\\n",
    "  --job-dir $JOB_DIR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached https://files.pythonhosted.org/packages/6a/49/7e10686647f741bd9c8918b0decdb94135b542fe372ca1100739b8529503/xgboost-0.82-py2.py3-none-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python2.7/site-packages (from xgboost)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python2.7/site-packages (from xgboost)\n",
      "Requirement already satisfied: mkl-random in /usr/local/lib/python2.7/dist-packages (from numpy->xgboost)\n",
      "Requirement already satisfied: mkl-fft in /usr/local/lib/python2.7/dist-packages (from numpy->xgboost)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python2.7/dist-packages (from numpy->xgboost)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python2.7/dist-packages (from numpy->xgboost)\n",
      "Requirement already satisfied: icc-rt in /usr/local/lib/python2.7/dist-packages (from numpy->xgboost)\n",
      "Requirement already satisfied: intel-numpy in /usr/local/lib/python2.7/dist-packages (from scipy->xgboost)\n",
      "Requirement already satisfied: intel-openmp in /usr/local/lib/python2.7/dist-packages (from mkl->numpy->xgboost)\n",
      "Requirement already satisfied: tbb==2019.* in /usr/local/lib/python2.7/dist-packages (from tbb4py->numpy->xgboost)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-0.82\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
