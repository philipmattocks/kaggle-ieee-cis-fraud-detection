{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT_ID=frauddetectionkaggle\n",
      "env: BUCKET_ID=frauddetectionkagglepkmatt\n",
      "env: JOB_DIR=gs://frauddetectionkagglepkmatt/xgb_job_dir\n",
      "env: REGION=europe-west1\n",
      "env: TRAINER_PACKAGE_PATH=./fraud_detection_hp_tuning\n",
      "env: MAIN_TRAINER_MODULE=fraud_detection_hp_tuning.train\n",
      "env: RUNTIME_VERSION=1.14\n",
      "env: PYTHON_VERSION=3.5\n",
      "env: HPTUNING_CONFIG=hptuning_config.yaml\n",
      "env: MODEL_NAME=fraud_detection_hp_tuning\n",
      "mkdir: cannot create directory ‘fraud_detection_hp_tuning’: File exists\n"
     ]
    }
   ],
   "source": [
    "%env PROJECT_ID frauddetectionkaggle\n",
    "%env BUCKET_ID frauddetectionkagglepkmatt\n",
    "%env JOB_DIR gs://frauddetectionkagglepkmatt/xgb_job_dir\n",
    "%env REGION europe-west1\n",
    "%env TRAINER_PACKAGE_PATH ./fraud_detection_hp_tuning\n",
    "%env MAIN_TRAINER_MODULE fraud_detection_hp_tuning.train\n",
    "%env RUNTIME_VERSION 1.14\n",
    "%env PYTHON_VERSION 3.5\n",
    "%env HPTUNING_CONFIG hptuning_config.yaml\n",
    "%env MODEL_NAME fraud_detection_hp_tuning\n",
    "! mkdir fraud_detection_hp_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./fraud_detection_hp_tuning/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./fraud_detection_hp_tuning/train.py\n",
    "import argparse\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "import hypertune\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.externals import joblib\n",
    "from random import shuffle\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "import category_encoders as ce\n",
    "identity = 'train_identity.csv'\n",
    "transaction = 'train_transaction.csv'\n",
    "\n",
    "BUCKET_ID = 'frauddetectionkagglepkmatt'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--job-dir',  # handled automatically by AI Platform\n",
    "    help='GCS location to write checkpoints and export models',\n",
    "    required=True\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--max_depth',  # Specified in the config file\n",
    "    help='Maximum depth of the XGBoost tree. default: 3',\n",
    "    default=3,\n",
    "    type=int\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--n_estimators',  # Specified in the config file\n",
    "    help='Number of estimators to be created. default: 100',\n",
    "    default=100,\n",
    "    type=int\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--booster',  # Specified in the config file\n",
    "    help='which booster to use: gbtree, gblinear or dart. default: gbtree',\n",
    "    default='gbtree',\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--learning_rate',  # Specified in the config file\n",
    "    help='what learning_rate to use: 0.05 typical',\n",
    "    default=0.05,\n",
    "    type=float\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--bin_or_numerical_class',  # Specified in the config file\n",
    "    help='whether to use binary or numerical label encoding for categoricals',\n",
    "    default='numerical',\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--extract_times',  # Specified in the config file\n",
    "    help='whether to use feature engineer time features',\n",
    "    default='true',\n",
    "    type=str\n",
    ")\n",
    "\n",
    "args = parser.parse_args()\n",
    "#  bucket holding the data\n",
    "bucket = storage.Client().bucket(BUCKET_ID)\n",
    "\n",
    "# Path to the data inside the public bucket\n",
    "data_dir = 'data/raw/'\n",
    "\n",
    "if not os.path.exists(identity):\n",
    "    # Download the data\n",
    "    blob = bucket.blob(''.join([data_dir, identity]))\n",
    "    blob.download_to_filename(identity)\n",
    "    \n",
    "if not os.path.exists(transaction):    \n",
    "    blob = bucket.blob(''.join([data_dir, transaction]))\n",
    "    blob.download_to_filename(transaction)\n",
    "\n",
    "\n",
    "def load_and_merge_data(transaction_csv,identity_csv,isTrain):\n",
    "    df_transaction = pd.read_csv(transaction_csv, index_col='TransactionID')\n",
    "    df_identity = pd.read_csv(identity_csv, index_col='TransactionID')\n",
    "    df = pd.merge(df_transaction, df_identity, on='TransactionID', how='left')\n",
    "    del df_transaction\n",
    "    del df_identity\n",
    "    if isTrain:\n",
    "        labels = df[['isFraud']]\n",
    "        df.pop('isFraud')\n",
    "    else:\n",
    "        labels = []\n",
    "    return df, labels\n",
    "\n",
    "train,labels  = load_and_merge_data(transaction,identity,isTrain=True)\n",
    "#train,labels  = load_and_merge_data('gs://frauddetectionkagglepkmatt/data/raw/train_transaction.csv','gs://frauddetectionkagglepkmatt/data/raw/train_identity.csv',isTrain=True,nrows=5000)\n",
    "# #validate,vallabels  = load_and_merge_data('./data/raw/test_transaction.csv','./data/raw/test_identity.csv',isTrain=False,nrows=5000)\n",
    "\n",
    "#print(train.shape)\n",
    "\n",
    "def get_lists_of_numerical_categorical(df,regex):\n",
    "    #Regex for categorical fields:\n",
    "    categorical = []\n",
    "    numerical = []\n",
    "\n",
    "    #Create lists of categorical and numeircal fields:\n",
    "    for i in df:\n",
    "        if re.match(regex, i):\n",
    "            categorical.append(i)\n",
    "        else:\n",
    "            numerical.append(i)\n",
    "    return numerical,categorical\n",
    "\n",
    "cat_columns_regex='ProductCD|card[1-6]|addr\\d|\\w_emaildomain|M[1-9]|time_|Device\\w+|id_12|id_13|id_14|id_15|id_16|id_17|id_18|id_19|id_20|id_21|id_22|id_23|id_24|id_25|id_26|id_27|id_28|id_29|id_30|id_31|id_32|id_33|id_34|id_35|id_36|id_37|id_38'\n",
    "numerical,categorical = get_lists_of_numerical_categorical(train,cat_columns_regex)\n",
    "\n",
    "def process_dates(df):\n",
    "    START_DATE = '2017-12-01'\n",
    "    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "    df['TransactionDT_converted'] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n",
    "    df['time_year'] = df['TransactionDT_converted'].dt.year\n",
    "    df['time_month'] = df['TransactionDT_converted'].dt.month\n",
    "    df['time_dow'] = df['TransactionDT_converted'].dt.dayofweek\n",
    "    df['time_hour'] = df['TransactionDT_converted'].dt.hour\n",
    "    df['time_day'] = df['TransactionDT_converted'].dt.day\n",
    "    df = df.drop(columns=\"TransactionDT_converted\")\n",
    "    df = df.drop(columns=\"time_year\")\n",
    "    df = df.drop(columns=\"time_month\")\n",
    "    df = df.drop(columns=\"time_day\")\n",
    "    return df\n",
    "\n",
    "if args.extract_times == 'true':\n",
    "    train = process_dates(train)\n",
    "\n",
    "if args.bin_or_numerical_class == 'numerical':\n",
    "    def numerically_encode_string_categoricals(df):\n",
    "        for i in df.columns:\n",
    "            if df[i].dtype == 'object':\n",
    "                lbl = preprocessing.LabelEncoder()\n",
    "                lbl.fit(list(df[i].values) + list(df[i].values))\n",
    "                df[i] = lbl.transform(list(df[i].values))\n",
    "        return df\n",
    "    train = numerically_encode_string_categoricals(train)\n",
    "    #validate = numerically_encode_string_categoricals(validate)\n",
    "    #Impute median for numerical and mode for categorical\n",
    "    def impute_cat_and_num(df,numerical,categorical):\n",
    "        fill_NaN_numerical = Imputer(missing_values=np.nan, strategy='median',axis=1)\n",
    "        fill_NaN_categorical = Imputer(missing_values=np.nan, strategy='most_frequent',axis=1)\n",
    "        df[numerical] = fill_NaN_numerical.fit_transform(df[numerical])\n",
    "        df[categorical] = fill_NaN_categorical.fit_transform(df[categorical])\n",
    "        return df\n",
    "    train = impute_cat_and_num(train,numerical,categorical)\n",
    "\n",
    "else:\n",
    "\n",
    "    def binary_encode_categoricals(df,categorical):\n",
    "        encoder = ce.BinaryEncoder(cols=categorical).fit(df)\n",
    "        df = encoder.transform(df)\n",
    "        return encoder,df\n",
    "    encoder,train = binary_encode_categoricals(train,categorical)\n",
    "    \n",
    "    \n",
    "    #Impute median for numerical and mode for categorical\n",
    "    def impute_cat_and_num(df,numerical,categorical):\n",
    "        fill_NaN_numerical = Imputer(missing_values=np.nan, strategy='median',axis=1)\n",
    "        #fill_NaN_categorical = Imputer(missing_values=np.nan, strategy='most_frequent',axis=1)\n",
    "        df[numerical] = fill_NaN_numerical.fit_transform(df[numerical])\n",
    "        #df[categorical] = fill_NaN_categorical.fit_transform(df[categorical])\n",
    "        return df\n",
    "    train = impute_cat_and_num(train,numerical,categorical)\n",
    "\n",
    "# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "# WARNING! THIS CAN DAMAGE THE DATA \n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.2,random_state=42)\n",
    "\n",
    "# Create the regressor, here we will use a Lasso Regressor to demonstrate the use of HP Tuning.\n",
    "# Here is where we set the variables used during HP Tuning from\n",
    "# the parameters passed into the python script\n",
    "\n",
    "#here are the tricks\n",
    "#usually max_depth is 6,7,8\n",
    "#learning rate is around 0.05, but small changes may make big diff\n",
    "#tuning min_child_weight subsample colsample_bytree can have \n",
    "#much fun of fighting against overfit \n",
    "#n_estimators is how many round of boosting\n",
    "#finally, ensemble xgboost with multiple seeds may reduce variance\n",
    "\n",
    "\n",
    "# clf = xgb.XGBClassifier(max_depth=args.max_depth,\n",
    "#                              n_estimators=args.n_estimators,\n",
    "#                              booster=args.booster,\n",
    "#                              nthread=7,\n",
    "#                              learning_rate=args.learning_rate\n",
    "#                             )\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "        nthread=7,\n",
    "        #n_estimators=1000,\n",
    "        n_estimators=args.n_estimators,\n",
    "        max_depth=args.max_depth,\n",
    "        #learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        tree_method='auto',\n",
    "        booster=args.booster,\n",
    "        random_state = 42,\n",
    "        learning_rate=args.learning_rate\n",
    "    )\n",
    "\n",
    "#clf = xgb.XGBClassifier()\n",
    "\n",
    "# Transform the features and fit them to the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Calculate the mean accuracy on the given test data and labels.\n",
    "#score = clf.score(X_test, y_test)\n",
    "\n",
    "#calculate the recall score on test data and labels\n",
    "#score = metrics.recall_score(y_test, clf.predict(X_test))\n",
    "\n",
    "#roc score\n",
    "score = metrics.roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])\n",
    "\n",
    "# The default name of the metric is training/hptuning/metric. \n",
    "# We recommend that you assign a custom name. The only functional difference is that \n",
    "# if you use a custom name, you must set the hyperparameterMetricTag value in the \n",
    "# HyperparameterSpec object in your job request to match your chosen name.\n",
    "# https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#HyperparameterSpec\n",
    "hpt = hypertune.HyperTune()\n",
    "hpt.report_hyperparameter_tuning_metric(\n",
    "   hyperparameter_metric_tag='my_metric_tag',\n",
    "   metric_value=score,\n",
    "   global_step=1000)\n",
    "\n",
    "# Export the model to a file\n",
    "model_filename = 'model.pkl'\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "# Example: job_dir = 'gs://BUCKET_ID/xgboost_job_dir/1'\n",
    "job_dir =  args.job_dir.replace('gs://', '')  # Remove the 'gs://'\n",
    "# Get the Bucket Id\n",
    "bucket_id = job_dir.split('/')[0]\n",
    "# Get the path\n",
    "bucket_path = job_dir[len('{}/'.format(bucket_id)):]  # Example: 'xgboost_job_dir/1'\n",
    "# Upload the model to GCS\n",
    "bucket = storage.Client().bucket(bucket_id)\n",
    "blob = bucket.blob('{}/{}'.format(\n",
    "    bucket_path,\n",
    "    model_filename))\n",
    "\n",
    "blob.upload_from_filename(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8214285714285714"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./fraud_detection_hp_tuning/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./fraud_detection_hp_tuning/__init__.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2018 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Note that __init__.py can be an empty file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./hptuning_config.yaml\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2018 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# hyperparam.yaml\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: complex_model_l\n",
    "  workerType: complex_model_l\n",
    "  workerCount: 3\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 18\n",
    "    maxParallelTrials: 3\n",
    "    hyperparameterMetricTag: my_metric_tag\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: max_depth\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 11\n",
    "    - parameterName: n_estimators\n",
    "      type: INTEGER\n",
    "      minValue: 500\n",
    "      maxValue: 1500\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.03\n",
    "      maxValue: 0.1\n",
    "    - parameterName: booster\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"gbtree\"]\n",
    "    - parameterName: bin_or_numerical_class\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"binary\"]\n",
    "    - parameterName: extract_times\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"true\"]\n",
    "\n",
    "# #to resume job\n",
    "# trainingInput:\n",
    "#   scaleTier: CUSTOM\n",
    "#   masterType: complex_model_l\n",
    "#   workerType: complex_model_l\n",
    "#   workerCount: 3\n",
    "#   hyperparameters:\n",
    "#     goal: MAXIMIZE\n",
    "#     maxTrials: 18\n",
    "#     maxParallelTrials: 6\n",
    "#     hyperparameterMetricTag: my_metric_tag\n",
    "#     enableTrialEarlyStopping: TRUE \n",
    "#     resumePreviousJobId: fraud_detection_hp_tuning_20190918_132014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./setup.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2018 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['cloudml-hypertune','category_encoders']\n",
    "\n",
    "setup(\n",
    "    name='fraud_detecion',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Auto MPG XGBoost HP tuning training application'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1.00 MB\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 100.0%\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "xgb_job_dir/model.pkl\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform local train \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --package-path $TRAINER_PACKAGE_PATH \\\n",
    "  --module-name $MAIN_TRAINER_MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":/home/jupyter/kaggle-ieee-cis-fraud-detection/GCP work/Hyperparameter_Tuning/fraud_detection_hp_tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] --job-dir JOB_DIR [--max_depth MAX_DEPTH]\n",
      "                [--n_estimators N_ESTIMATORS] [--booster BOOSTER]\n",
      "train.py: error: unrecognized arguments: --config hptuning_config.yaml\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b\"# Setup python so it sees the task module which controls the model.py\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/${MODEL_NAME}\\necho ${PYTHONPATH}\\n# Currently set for python 3.  To run with python 2\\n#    1.  Replace 'python3' with 'python' in the following command\\n#    2.  Edit trainer/task.py to reflect proper module import method \\npython3 -m $MAIN_TRAINER_MODULE \\\\\\n  --job-dir $JOB_DIR  \\\\\\n  --config $HPTUNING_CONFIG\\n\"' returned non-zero exit status 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-aeb8723b58a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"# Setup python so it sees the task module which controls the model.py\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/${MODEL_NAME}\\necho ${PYTHONPATH}\\n# Currently set for python 3.  To run with python 2\\n#    1.  Replace 'python3' with 'python' in the following command\\n#    2.  Edit trainer/task.py to reflect proper module import method \\npython3 -m $MAIN_TRAINER_MODULE \\\\\\n  --job-dir $JOB_DIR  \\\\\\n  --config $HPTUNING_CONFIG\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</usr/local/lib/python3.5/dist-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b\"# Setup python so it sees the task module which controls the model.py\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/${MODEL_NAME}\\necho ${PYTHONPATH}\\n# Currently set for python 3.  To run with python 2\\n#    1.  Replace 'python3' with 'python' in the following command\\n#    2.  Edit trainer/task.py to reflect proper module import method \\npython3 -m $MAIN_TRAINER_MODULE \\\\\\n  --job-dir $JOB_DIR  \\\\\\n  --config $HPTUNING_CONFIG\\n\"' returned non-zero exit status 2"
     ]
    }
   ],
   "source": [
    "#  %%bash\n",
    "#  # Setup python so it sees the task module which controls the model.py\n",
    "#  export PYTHONPATH=${PYTHONPATH}:${PWD}/${MODEL_NAME}\n",
    "#  echo ${PYTHONPATH}\n",
    "#  # Currently set for python 3.  To run with python 2\n",
    "#  #    1.  Replace 'python3' with 'python' in the following command\n",
    "#  #    2.  Edit trainer/task.py to reflect proper module import method \n",
    "#  python3 -m $MAIN_TRAINER_MODULE \\\n",
    "#    --job-dir $JOB_DIR  \\\n",
    "#    --config $HPTUNING_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [fraud_detection_hp_tuning_20190924_120521] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe fraud_detection_hp_tuning_20190924_120521\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs fraud_detection_hp_tuning_20190924_120521\n",
      "jobId: fraud_detection_hp_tuning_20190924_120521\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform jobs submit training fraud_detection_hp_tuning_$(date +\"%Y%m%d_%H%M%S\") \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --package-path $TRAINER_PACKAGE_PATH \\\n",
    "  --module-name $MAIN_TRAINER_MODULE \\\n",
    "  --region $REGION \\\n",
    "  --runtime-version=$RUNTIME_VERSION \\\n",
    "  --python-version=$PYTHON_VERSION \\\n",
    "  --config $HPTUNING_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2019-09-17T15:33:43Z'\n",
      "endTime: '2019-09-17T16:09:14Z'\n",
      "etag: d9bR9zT0eG4=\n",
      "jobId: fraud_detection_hp_tuning_20190917_153340\n",
      "startTime: '2019-09-17T15:33:52Z'\n",
      "state: SUCCEEDED\n",
      "trainingInput:\n",
      "  hyperparameters:\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: my_metric_tag\n",
      "    maxParallelTrials: 5\n",
      "    maxTrials: 30\n",
      "    params:\n",
      "    - maxValue: 8.0\n",
      "      minValue: 3.0\n",
      "      parameterName: max_depth\n",
      "      type: INTEGER\n",
      "    - maxValue: 200.0\n",
      "      minValue: 50.0\n",
      "      parameterName: n_estimators\n",
      "      type: INTEGER\n",
      "    - categoricalValues:\n",
      "      - gbtree\n",
      "      parameterName: booster\n",
      "      type: CATEGORICAL\n",
      "  jobDir: gs://frauddetectionkagglepkm/xgb_job_dir\n",
      "  packageUris:\n",
      "  - gs://frauddetectionkagglepkm/xgb_job_dir/packages/1ae8f37238318317342a4a145396435ea00ada3e8ef9f95f8cda2cfd7eb1aaba/fraud_detecion-0.1.tar.gz\n",
      "  pythonModule: fraud_detection_hp_tuning.train\n",
      "  pythonVersion: '3.5'\n",
      "  region: europe-west1\n",
      "  runtimeVersion: '1.14'\n",
      "trainingOutput:\n",
      "  completedTrialCount: '30'\n",
      "  consumedMLUnits: 2.03\n",
      "  hyperparameterMetricTag: my_metric_tag\n",
      "  isHyperparameterTuningJob: true\n",
      "  trials:\n",
      "  - endTime: '2019-09-17T15:56:57Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '3'\n",
      "      n_estimators: '178'\n",
      "    startTime: '2019-09-17T15:52:11.665199077Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '19'\n",
      "  - endTime: '2019-09-17T15:56:56Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '8'\n",
      "      n_estimators: '50'\n",
      "    startTime: '2019-09-17T15:52:11.665377601Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '20'\n",
      "  - endTime: '2019-09-17T16:02:25Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '3'\n",
      "      n_estimators: '113'\n",
      "    startTime: '2019-09-17T15:57:40.912148431Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '21'\n",
      "  - endTime: '2019-09-17T16:02:27Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '8'\n",
      "      n_estimators: '200'\n",
      "    startTime: '2019-09-17T15:57:40.912204935Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '22'\n",
      "  - endTime: '2019-09-17T16:02:26Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '8'\n",
      "      n_estimators: '60'\n",
      "    startTime: '2019-09-17T15:57:40.911812965Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '23'\n",
      "  - endTime: '2019-09-17T16:02:29Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '8'\n",
      "      n_estimators: '156'\n",
      "    startTime: '2019-09-17T15:57:40.911995328Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '24'\n",
      "  - endTime: '2019-09-17T16:02:29Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '3'\n",
      "      n_estimators: '85'\n",
      "    startTime: '2019-09-17T15:57:40.912085620Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '25'\n",
      "  - endTime: '2019-09-17T16:08:36Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '8'\n",
      "      n_estimators: '125'\n",
      "    startTime: '2019-09-17T16:03:50.263097129Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '26'\n",
      "  - endTime: '2019-09-17T16:08:36Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '3'\n",
      "      n_estimators: '50'\n",
      "    startTime: '2019-09-17T16:03:50.263244105Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '27'\n",
      "  - endTime: '2019-09-17T16:08:34Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '3'\n",
      "      n_estimators: '200'\n",
      "    startTime: '2019-09-17T16:03:50.263310015Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '28'\n",
      "  - endTime: '2019-09-17T16:08:35Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '6'\n",
      "      n_estimators: '79'\n",
      "    startTime: '2019-09-17T16:03:50.263380595Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '29'\n",
      "  - endTime: '2019-09-17T16:08:34Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '8'\n",
      "      n_estimators: '163'\n",
      "    startTime: '2019-09-17T16:03:50.263444006Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '30'\n",
      "  - endTime: '2019-09-17T15:38:43Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '6'\n",
      "      n_estimators: '99'\n",
      "    startTime: '2019-09-17T15:33:57.619325034Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '1'\n",
      "  - endTime: '2019-09-17T15:38:44Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '4'\n",
      "      n_estimators: '190'\n",
      "    startTime: '2019-09-17T15:33:57.619462339Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '2'\n",
      "  - endTime: '2019-09-17T15:38:43Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '8'\n",
      "      n_estimators: '69'\n",
      "    startTime: '2019-09-17T15:33:57.619514337Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '3'\n",
      "  - endTime: '2019-09-17T15:38:46Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '6'\n",
      "      n_estimators: '188'\n",
      "    startTime: '2019-09-17T15:33:57.619559026Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '4'\n",
      "  - endTime: '2019-09-17T15:38:42Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '4'\n",
      "      n_estimators: '128'\n",
      "    startTime: '2019-09-17T15:33:57.619600359Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '5'\n",
      "  - endTime: '2019-09-17T15:44:58Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '4'\n",
      "      n_estimators: '152'\n",
      "    startTime: '2019-09-17T15:40:12.991667333Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '6'\n",
      "  - endTime: '2019-09-17T15:44:57Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '8'\n",
      "      n_estimators: '92'\n",
      "    startTime: '2019-09-17T15:40:12.991805130Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '7'\n",
      "  - endTime: '2019-09-17T15:45:30Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '6'\n",
      "      n_estimators: '122'\n",
      "    startTime: '2019-09-17T15:40:12.991848472Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '8'\n",
      "  - endTime: '2019-09-17T15:45:00Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '4'\n",
      "      n_estimators: '115'\n",
      "    startTime: '2019-09-17T15:40:12.991884104Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '9'\n",
      "  - endTime: '2019-09-17T15:45:00Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '4'\n",
      "      n_estimators: '164'\n",
      "    startTime: '2019-09-17T15:40:12.991928769Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '10'\n",
      "  - endTime: '2019-09-17T15:50:57Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '4'\n",
      "      n_estimators: '109'\n",
      "    startTime: '2019-09-17T15:46:13.338990290Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '11'\n",
      "  - endTime: '2019-09-17T15:50:59Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '8'\n",
      "      n_estimators: '200'\n",
      "    startTime: '2019-09-17T15:46:13.339205939Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '12'\n",
      "  - endTime: '2019-09-17T15:51:00Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '8'\n",
      "      n_estimators: '50'\n",
      "    startTime: '2019-09-17T15:46:13.339348521Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '13'\n",
      "  - endTime: '2019-09-17T15:50:57Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '3'\n",
      "      n_estimators: '156'\n",
      "    startTime: '2019-09-17T15:46:13.339393722Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '14'\n",
      "  - endTime: '2019-09-17T15:50:59Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '4'\n",
      "      n_estimators: '178'\n",
      "    startTime: '2019-09-17T15:46:13.339286254Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '15'\n",
      "  - endTime: '2019-09-17T15:56:58Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '4'\n",
      "      n_estimators: '84'\n",
      "    startTime: '2019-09-17T15:52:11.665453931Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '16'\n",
      "  - endTime: '2019-09-17T15:56:26Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '8'\n",
      "      n_estimators: '200'\n",
      "    startTime: '2019-09-17T15:52:11.665494362Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '17'\n",
      "  - endTime: '2019-09-17T15:56:26Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 1.0\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '8'\n",
      "      n_estimators: '143'\n",
      "    startTime: '2019-09-17T15:52:11.665533221Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '18'\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/fraud_detection_hp_tuning_20190917_153340?project=frauddetectionkaggle\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2Ffraud_detection_hp_tuning_20190917_153340&project=frauddetectionkaggle\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform jobs describe fraud_detection_hp_tuning_20190917_153340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 165.00 MB\n",
      "Memory usage after optimization is: 42.00 MB\n",
      "Decreased by 74.0%\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/home/jupyter/kaggle-ieee-cis-fraud-detection/GCP work/Hyperparameter_Tuning/fraud_detection_hp_tuning/train.py\", line 198, in <module>\n",
      "    score = metrics.recall_score(y_test, y_pred)\n",
      "NameError: name 'metrics' is not defined\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform local train \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --package-path $TRAINER_PACKAGE_PATH \\\n",
    "  --module-name $MAIN_TRAINER_MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":/home/jupyter/kaggle-ieee-cis-fraud-detection/fraud_detection_hp_tuning\n",
      "Memory usage of dataframe is 1.65 MB\n",
      "Memory usage after optimization is: 0.40 MB\n",
      "Decreased by 75.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Setup python so it sees the task module which controls the model.py\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/${MODEL_NAME}\n",
    "echo ${PYTHONPATH}\n",
    "# Currently set for python 3.  To run with python 2\n",
    "#    1.  Replace 'python3' with 'python' in the following command\n",
    "#    2.  Edit trainer/task.py to reflect proper module import method \n",
    "python3 -m $MAIN_TRAINER_MODULE \\\n",
    "  --job-dir $JOB_DIR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test on created model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.20.2 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump,load\n",
    "bucket = storage.Client().bucket(BUCKET_ID)\n",
    "filename='model.pkl'\n",
    "# Path to the data inside the public bucket\n",
    "data_dir = 'xgb_job_dir/4/'\n",
    "# Download the data\n",
    "blob = bucket.blob(''.join([data_dir, filename]))\n",
    "blob.download_to_filename(filename)\n",
    "clf=load('./model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "y_pred_prob=xgbm_old.predict_proba(X_test)\n",
    "y_pred=xgbm_old.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"ROC area under:\",metrics.roc_auc_score(y_test, y_pred_prob[:,1]))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"Balanced Accuracy:\",metrics.balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"Precision Score:\",metrics.precision_score(y_test, y_pred,pos_label=1,average='binary'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
