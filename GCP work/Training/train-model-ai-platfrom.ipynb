{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set env variables and create directory for module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT_ID=frauddetectionkaggle\n",
      "env: BUCKET_ID=frauddetectionkagglepkmatt\n",
      "env: REGION=europe-west1\n",
      "env: TRAINER_PACKAGE_PATH=./fraud_detection_training\n",
      "env: MAIN_TRAINER_MODULE=fraud_detection_training.train\n",
      "env: JOB_DIR=gs://frauddetectionkagglepkm/xgb_job_dir\n",
      "env: RUNTIME_VERSION=1.14\n",
      "env: PYTHON_VERSION=3.5\n",
      "mkdir: cannot create directory ‘fraud_detection_training’: File exists\n"
     ]
    }
   ],
   "source": [
    "%env PROJECT_ID frauddetectionkaggle\n",
    "%env BUCKET_ID frauddetectionkagglepkmatt\n",
    "%env REGION europe-west1\n",
    "%env TRAINER_PACKAGE_PATH ./fraud_detection_training\n",
    "%env MAIN_TRAINER_MODULE fraud_detection_training.train\n",
    "%env JOB_DIR gs://frauddetectionkagglepkm/xgb_job_dir\n",
    "%env RUNTIME_VERSION 1.14\n",
    "%env PYTHON_VERSION 3.5\n",
    "! mkdir fraud_detection_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a bucket (NB: AI Platform won't work with a multi region bucket!!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://frauddetectionkagglepkmatt/...\n",
      "ServiceException: 409 Bucket frauddetectionkagglepkmatt already exists.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION gs://$BUCKET_ID\n",
    "#upload the CSVs to this bucket manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create training script that downloads, encodes, trains and saves the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1.65 MB\n",
      "Memory usage after optimization is: 0.40 MB\n",
      "Decreased by 75.5%\n"
     ]
    }
   ],
   "source": [
    "#%%writefile ./fraud_detection_training/train.py\n",
    "# [START setup]\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb\n",
    "from google.cloud import storage\n",
    "import os\n",
    "BUCKET_ID = 'frauddetectionkagglepkmatt'\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Add code to download the data from GCS \n",
    "# AI Platform will then be able to use the data when training your model.\n",
    "# ---------------------------------------\n",
    "# [START download-data]\n",
    "\n",
    "identity = 'train_identity_small.csv'\n",
    "transaction = 'train_transaction_small.csv'\n",
    "\n",
    "#  bucket holding the data\n",
    "bucket = storage.Client().bucket(BUCKET_ID)\n",
    "\n",
    "# Path to the data inside the public bucket\n",
    "data_dir = 'data/raw/'\n",
    "\n",
    "if not os.path.exists(identity):\n",
    "    # Download the data\n",
    "    blob = bucket.blob(''.join([data_dir, identity]))\n",
    "    blob.download_to_filename(identity)\n",
    "    \n",
    "if not os.path.exists(transaction):    \n",
    "    blob = bucket.blob(''.join([data_dir, transaction]))\n",
    "    blob.download_to_filename(transaction)\n",
    "\n",
    "\n",
    "def load_and_merge_data(transaction_csv,identity_csv,isTrain,nrows=1000000):\n",
    "    df_transaction = pd.read_csv(transaction_csv, index_col='TransactionID',nrows=nrows)\n",
    "    df_identity = pd.read_csv(identity_csv, index_col='TransactionID',nrows=nrows)\n",
    "    df = pd.merge(df_transaction, df_identity, on='TransactionID', how='left')\n",
    "    del df_transaction\n",
    "    del df_identity\n",
    "    if isTrain:\n",
    "        labels = df[['isFraud']]\n",
    "        df.pop('isFraud')\n",
    "    else:\n",
    "        labels = []\n",
    "    return df, labels\n",
    "\n",
    "train,labels  = load_and_merge_data(transaction,identity,isTrain=True)\n",
    "#train,labels  = load_and_merge_data('gs://frauddetectionkagglepkmatt/data/raw/train_transaction.csv','gs://frauddetectionkagglepkmatt/data/raw/train_identity.csv',isTrain=True,nrows=5000)\n",
    "# #validate,vallabels  = load_and_merge_data('./data/raw/test_transaction.csv','./data/raw/test_identity.csv',isTrain=False,nrows=5000)\n",
    "\n",
    "#print(train.shape)\n",
    "\n",
    "def get_lists_of_numerical_categorical(df,regex):\n",
    "    #Regex for categorical fields:\n",
    "    categorical = []\n",
    "    numerical = []\n",
    "\n",
    "    #Create lists of categorical and numeircal fields:\n",
    "    for i in df:\n",
    "        if re.match(regex, i):\n",
    "            categorical.append(i)\n",
    "        else:\n",
    "            numerical.append(i)\n",
    "    return numerical,categorical\n",
    "\n",
    "cat_columns_regex='ProductCD|card[1-6]|addr\\d|\\w_emaildomain|M[1-9]|time_|Device\\w+|id_12|id_13|id_14|id_15|id_16|id_17|id_18|id_19|id_20|id_21|id_22|id_23|id_24|id_25|id_26|id_27|id_28|id_29|id_30|id_31|id_32|id_33|id_34|id_35|id_36|id_37|id_38'\n",
    "numerical,categorical = get_lists_of_numerical_categorical(train,cat_columns_regex)\n",
    "\n",
    "def numerically_encode_string_categoricals(df):\n",
    "    for i in df.columns:\n",
    "        if df[i].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(df[i].values) + list(df[i].values))\n",
    "            df[i] = lbl.transform(list(df[i].values))\n",
    "    return df\n",
    "train = numerically_encode_string_categoricals(train)\n",
    "#validate = numerically_encode_string_categoricals(validate)\n",
    "\n",
    "# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "# WARNING! THIS CAN DAMAGE THE DATA \n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "\n",
    "#Impute median for numerical and mode for categorical\n",
    "def impute_cat_and_num(df,numerical,categorical):\n",
    "    fill_NaN_numerical = Imputer(missing_values=np.nan, strategy='median',axis=1)\n",
    "    fill_NaN_categorical = Imputer(missing_values=np.nan, strategy='most_frequent',axis=1)\n",
    "    df[numerical] = fill_NaN_numerical.fit_transform(df[numerical])\n",
    "    df[categorical] = fill_NaN_categorical.fit_transform(df[categorical])\n",
    "    return df\n",
    "train = impute_cat_and_num(train,numerical,categorical)\n",
    "\n",
    "# [START load-into-dmatrix-and-train]\n",
    "# load data into DMatrix object\n",
    "dtrain = xgb.DMatrix(train, labels)\n",
    "\n",
    "# train model\n",
    "\n",
    "param_dict = {\n",
    "    'base_score':0.5,\n",
    "    'booster':'gbtree',\n",
    "    'colsample_bylevel':1,\n",
    "    'colsample_bynode':1,\n",
    "    'colsample_bytree':0.9,\n",
    "    'gamma':0,\n",
    "    'learning_rate':0.05,\n",
    "    'max_delta_step':0,\n",
    "    'max_depth':9,\n",
    "    'min_child_weight':1,\n",
    "    'n_estimators':1000,#this has to be entered explicitly in the function\n",
    "    'n_jobs':1,\n",
    "    'nthread':7,\n",
    "    'objective':'binary:logistic',\n",
    "    'random_state':42,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':1,\n",
    "    'scale_pos_weight':1,\n",
    "    'seed':42,\n",
    "    'subsample':0.9,\n",
    "    'tree_method':'auto',\n",
    "    'verbosity':1\n",
    "}\n",
    "\n",
    "clf = xgb.train(param_dict, dtrain, param_dict['n_estimators'])\n",
    "model = 'model.joblib'\n",
    "#clf.save_model(model)\n",
    "# Export the model to a file\n",
    "joblib.dump(clf, model)\n",
    "# ---------------------------------------\n",
    "# 2. Export and save the model to GCS\n",
    "# ---------------------------------------\n",
    "bucket = storage.Client().bucket(BUCKET_ID)\n",
    "blob = bucket.blob('{}/{}'.format(\n",
    "    datetime.datetime.now().strftime('fraud_detect_kaggle_%Y%m%d_%H%M%S'),\n",
    "    model))\n",
    "blob.upload_from_filename(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./fraud_detection_training/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./fraud_detection_training/__init__.py\n",
    "# Note that __init__.py can be an empty file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "\u001b[1;33mWARNING:\u001b[0m You do not appear to have access to project [frauddetectionkaggle] or it does not exist.\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./fraud_detection_training/setup.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile ./fraud_detection_training/setup.py\n",
    "#from setuptools import find_packages\n",
    "#from setuptools import setup\n",
    "#\n",
    "#REQUIRED_PACKAGES = ['xgboost']\n",
    "#\n",
    "#setup(\n",
    "#    name='trainer',\n",
    "#    version='0.1',\n",
    "#    install_requires=REQUIRED_PACKAGES,\n",
    "#    packages=find_packages(),\n",
    "#    include_package_data=True,\n",
    "#    description='My training application package.'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(499, 432)\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform local train \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --package-path $TRAINER_PACKAGE_PATH \\\n",
    "  --module-name $MAIN_TRAINER_MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [fraud_detection_training_20190916_164725] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe fraud_detection_training_20190916_164725\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs fraud_detection_training_20190916_164725\n",
      "jobId: fraud_detection_training_20190916_164725\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform jobs submit training fraud_detection_training_$(date +\"%Y%m%d_%H%M%S\") \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --package-path $TRAINER_PACKAGE_PATH \\\n",
    "  --module-name $MAIN_TRAINER_MODULE \\\n",
    "  --region $REGION \\\n",
    "  --runtime-version=$RUNTIME_VERSION \\\n",
    "  --python-version=$PYTHON_VERSION \\\n",
    "  --scale-tier BASIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'model.bst'\n",
    "#  bucket holding the data\n",
    "bucket = storage.Client().bucket(BUCKET_ID)\n",
    "data_dir = 'fraud_detect_kaggle_20190916_185055/'\n",
    "#gs://frauddetectionkagglepkmatt/fraud_detect_kaggle_20190916_185055/model.bst\n",
    "# Download the data\n",
    "blob = bucket.blob(''.join([data_dir, model]))\n",
    "blob.download_to_filename(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('./model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate,vallabels  = load_and_merge_data('./data/raw/test_transaction.csv','./data/raw/test_identity.csv',isTrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1673.87 MB\n",
      "Memory usage after optimization is: 460.02 MB\n",
      "Decreased by 72.5%\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-1ba3d4dc9d73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvalidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerically_encode_string_categoricals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mem_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvalidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimpute_cat_and_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumerical\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdvalidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-252-256481d2b602>\u001b[0m in \u001b[0;36mimpute_cat_and_num\u001b[0;34m(df, numerical, categorical)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mfill_NaN_numerical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'median'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mfill_NaN_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'most_frequent'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumerical\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_NaN_numerical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumerical\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_NaN_categorical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/sklearn/preprocessing/imputation.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES,\n\u001b[0;32m--> 323\u001b[0;31m                             force_all_finite=False, copy=self.copy)\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cat_columns_regex='ProductCD|card[1-6]|addr\\d|\\w_emaildomain|M[1-9]|time_|Device\\w+|id_12|id_13|id_14|id_15|id_16|id_17|id_18|id_19|id_20|id_21|id_22|id_23|id_24|id_25|id_26|id_27|id_28|id_29|id_30|id_31|id_32|id_33|id_34|id_35|id_36|id_37|id_38'\n",
    "numerical,categorical = get_lists_of_numerical_categorical(validate,cat_columns_regex)\n",
    "validate = numerically_encode_string_categoricals(validate)\n",
    "validate = reduce_mem_usage(validate)\n",
    "validate = impute_cat_and_num(validate,numerical,categorical)\n",
    "dvalidate = xgb.DMatrix(validate, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred=clf.predict(dvalidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for API:\n",
    "#validate = {\"instances\": validate.values.tolist()}\n",
    "#for gcloud:\n",
    "validate = validate.values.tolist()\n",
    "with open(\"test.txt\", \"w\") as output:\n",
    "    output.write(str(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the signature defined in the model is not serving_default then you must specify it via --signature-name flag, otherwise the command may fail.\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.local.predict) Cannot import xgboost. Please verify \"python -c 'import xgboost'\" works.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai-platform local predict --model-dir gs://frauddetectionkagglepkmatt/fraud_detect_kaggle_20190917_090223/ \\\n",
    "  --text-instances test.txt \\\n",
    "  --framework xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
