{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple data processing, no feature engineering, just encoding categoricals and imputing NAs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FraudKagglePreProcessData import PreProcessData\n",
    "from datetime import datetime\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "import os\n",
    "from joblib import dump\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Loading and prep <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_data(transaction_csv,identity_csv,isTrain):\n",
    "    df_transaction = pd.read_csv(transaction_csv, index_col='TransactionID')\n",
    "    df_identity = pd.read_csv(identity_csv, index_col='TransactionID')\n",
    "    df = pd.merge(df_transaction, df_identity, on='TransactionID', how='left')\n",
    "    del df_transaction\n",
    "    del df_identity\n",
    "    if isTrain:\n",
    "        labels = df[['isFraud']]\n",
    "        df.pop('isFraud')\n",
    "    else:\n",
    "        labels = []\n",
    "    return df, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0:01:11.474487\n"
     ]
    }
   ],
   "source": [
    "start=datetime.now()\n",
    "train,labels  = load_and_merge_data('./data/raw/train_transaction.csv','./data/raw/train_identity.csv',isTrain=True)\n",
    "validate,vallabels  = load_and_merge_data('./data/raw/test_transaction.csv','./data/raw/test_identity.csv',isTrain=False)\n",
    "finish=datetime.now()\n",
    "print('time taken: '+str(finish-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_pickle('./data/interim/train_joined.pkl')\n",
    "#test.to_pickle('./data/interim/test_joined.pkl')\n",
    "#labels.to_pickle('./data/interim/labels.pkl')\n",
    "#train = pd.read_pickle('./data/interim/train_joined.pkl')\n",
    "#test = pd.read_pickle('./data/interim/test_joined.pkl')\n",
    "#labels = pd.read_pickle('./data/interim/labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0:00:00.002988\n"
     ]
    }
   ],
   "source": [
    "def get_lists_of_numerical_categorical(df,regex):\n",
    "    #Regex for categorical fields:\n",
    "    categorical = []\n",
    "    numerical = []\n",
    "\n",
    "    #Create lists of categorical and numeircal fields:\n",
    "    for i in df:\n",
    "        if re.match(regex, i):\n",
    "            categorical.append(i)\n",
    "        else:\n",
    "            numerical.append(i)\n",
    "    return numerical,categorical\n",
    "start=datetime.now()\n",
    "cat_columns_regex='ProductCD|card[1-6]|addr\\d|\\w_emaildomain|M[1-9]|time_|Device\\w+|id_12|id_13|id_14|id_15|id_16|id_17|id_18|id_19|id_20|id_21|id_22|id_23|id_24|id_25|id_26|id_27|id_28|id_29|id_30|id_31|id_32|id_33|id_34|id_35|id_36|id_37|id_38'\n",
    "numerical,categorical = get_lists_of_numerical_categorical(train,cat_columns_regex)\n",
    "finish=datetime.now()\n",
    "print('time taken: '+str(finish-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0:03:27.677484\n"
     ]
    }
   ],
   "source": [
    "start=datetime.now()\n",
    "def numerically_encode_string_categoricals(df):\n",
    "    for i in df.columns:\n",
    "        if df[i].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(df[i].values) + list(df[i].values))\n",
    "            df[i] = lbl.transform(list(df[i].values))\n",
    "    return df\n",
    "train = numerically_encode_string_categoricals(train)\n",
    "validate = numerically_encode_string_categoricals(validate)\n",
    "finish=datetime.now()\n",
    "print('time taken: '+str(finish-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "# WARNING! THIS CAN DAMAGE THE DATA \n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1950.87 MB\n",
      "Memory usage after optimization is: 527.14 MB\n",
      "Decreased by 73.0%\n",
      "time taken: 0:03:37.732491\n"
     ]
    }
   ],
   "source": [
    "start=datetime.now()\n",
    "train = reduce_mem_usage(train)\n",
    "finish=datetime.now()\n",
    "print('time taken: '+str(finish-start))\n",
    "#validate = reduce_mem_usage(validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute missing values on numerical features with median and categorical with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0:05:46.375678\n"
     ]
    }
   ],
   "source": [
    "start=datetime.now()\n",
    "def impute_cat_and_num(df,numerical,categorical):\n",
    "    fill_NaN_numerical = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    fill_NaN_categorical = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    df[numerical] = fill_NaN_numerical.fit_transform(df[numerical])\n",
    "    df[categorical] = fill_NaN_categorical.fit_transform(df[categorical])\n",
    "    return df\n",
    "train = impute_cat_and_num(train,numerical,categorical)\n",
    "validate = impute_cat_and_num(validate,numerical,categorical)\n",
    "finish=datetime.now()\n",
    "print('time taken: '+str(finish-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1950.87 MB\n",
      "Memory usage after optimization is: 542.91 MB\n",
      "Decreased by 72.2%\n",
      "time taken: 0:03:17.304864\n"
     ]
    }
   ],
   "source": [
    "start=datetime.now()\n",
    "train = reduce_mem_usage(train)\n",
    "finish=datetime.now()\n",
    "print('time taken: '+str(finish-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_pickle('./data/interim/train_joined.pkl')\n",
    "#test.to_pickle('./data/interim/test_joined.pkl')\n",
    "#labels.to_pickle('./data/interim/labels.pkl')\n",
    "#train = pd.read_pickle('./data/interim/train_joined.pkl')\n",
    "#test = pd.read_pickle('./data/interim/test_joined.pkl')\n",
    "#labels = pd.read_pickle('./data/interim/labels.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train into train and test for validating initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0:00:03.531045\n"
     ]
    }
   ],
   "source": [
    "start=datetime.now()\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.2,random_state=42)\n",
    "finish=datetime.now()\n",
    "print('time taken: '+str(finish-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Random Forest <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0:00:00.026927\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V203</th>\n",
       "      <td>0.071066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V186</th>\n",
       "      <td>0.066499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V232</th>\n",
       "      <td>0.059997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_13</th>\n",
       "      <td>0.052245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V123</th>\n",
       "      <td>0.048959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "V203   0.071066\n",
       "V186   0.066499\n",
       "V232   0.059997\n",
       "id_13  0.052245\n",
       "V123   0.048959"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42,n_estimators = 100, max_depth=25)\n",
    "rf.fit(X_train, y_train)\n",
    "finish=datetime.now()\n",
    "print('time taken: '+str(finish-start))\n",
    "important_features = pd.DataFrame(rf.feature_importances_,features).sort_values(0,ascending=False)\n",
    "important_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99\n",
      "ROC area under: 0.7244897959183674\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob=rf.predict_proba(X_test)\n",
    "y_pred=rf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"ROC area under:\",metrics.roc_auc_score(y_test, y_pred_prob[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.read_pickle('./data/interim/grid_search_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid search using cross validation for hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    2.1s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    2.3s finished\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state = 42)\n",
    "param = {'n_estimators': [300, 500],\n",
    "        'max_depth': [50]}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=3, n_jobs=-1,scoring='roc_auc',verbose=2)\n",
    "gs_fit = gs.fit(X_train, y_train)\n",
    "\n",
    "results = pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)\n",
    "\n",
    "if 'all_results' in locals():\n",
    "    all_results = pd.concat([all_results,results])\n",
    "else:\n",
    "    all_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 300}</td>\n",
       "      <td>0.462763</td>\n",
       "      <td>0.973303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 300}</td>\n",
       "      <td>0.234372</td>\n",
       "      <td>0.973303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 300}</td>\n",
       "      <td>0.222405</td>\n",
       "      <td>0.973303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 300}</td>\n",
       "      <td>0.233708</td>\n",
       "      <td>0.973303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 500}</td>\n",
       "      <td>0.728053</td>\n",
       "      <td>0.971394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 500}</td>\n",
       "      <td>0.366038</td>\n",
       "      <td>0.971394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 500}</td>\n",
       "      <td>0.349732</td>\n",
       "      <td>0.971394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 500}</td>\n",
       "      <td>0.369346</td>\n",
       "      <td>0.971394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   params  mean_fit_time  mean_test_score\n",
       "0  {'max_depth': 50, 'n_estimators': 300}       0.462763         0.973303\n",
       "0  {'max_depth': 50, 'n_estimators': 300}       0.234372         0.973303\n",
       "0  {'max_depth': 50, 'n_estimators': 300}       0.222405         0.973303\n",
       "0  {'max_depth': 50, 'n_estimators': 300}       0.233708         0.973303\n",
       "1  {'max_depth': 50, 'n_estimators': 500}       0.728053         0.971394\n",
       "1  {'max_depth': 50, 'n_estimators': 500}       0.366038         0.971394\n",
       "1  {'max_depth': 50, 'n_estimators': 500}       0.349732         0.971394\n",
       "1  {'max_depth': 50, 'n_estimators': 500}       0.369346         0.971394"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[['params','mean_fit_time','mean_test_score']].sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.to_pickle('./data/interim/grid_search_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train new model with best hyper parameters from above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "rf = RandomForestClassifier(random_state=42,n_estimators = 500, max_depth=25)\n",
    "rf.fit(train, labels)\n",
    "finish=datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get predictions on un-labelled data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred=rf.predict_proba(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/predictions/13-09-2019_12-15-36/random_foest_base.joblib']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetimestring=dt.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "os.mkdir('./data/predictions/'+datetimestring)\n",
    "validate['isFraud'] = val_pred[:,1]\n",
    "validate['TransactionID']=list(validate.index)\n",
    "validate[['TransactionID','isFraud']].to_csv('./data/predictions/'+datetimestring+'/prediction_results.csv',index = False)\n",
    "dump(rf, './data/predictions/'+datetimestring+'/random_foest_base.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>XGBoost<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=datetime.now()\n",
    "xgb = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=9,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "finish=datetime.now()\n",
    "print('time taken: '+str(finish-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99\n",
      "ROC area under: 0.8316326530612245\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob=xgb.predict_proba(X_test)\n",
    "y_pred=xgb.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"ROC area under:\",metrics.roc_auc_score(y_test, y_pred_prob[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate=validate.drop(['isFraud','TransactionID'], axis=1)\n",
    "val_pred=xgb.predict_proba(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/predictions/13-09-2019_12-15-38/XGBoost.joblib']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetimestring=dt.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "os.mkdir('./data/predictions/'+datetimestring)\n",
    "validate['isFraud'] = val_pred[:,1]\n",
    "validate['TransactionID']=list(validate.index)\n",
    "validate[['TransactionID','isFraud']].to_csv('./data/predictions/'+datetimestring+'/prediction_results.csv',index = False)\n",
    "dump(rf, './data/predictions/'+datetimestring+'/XGBoost.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
